---
description: Guidelines for using PocketFlow, Utility Function, Text Chunking
globs: 
alwaysApply: false
---
# Text Chunking

We recommend some implementations of commonly used text chunking approaches.


> Text Chunking is more a micro optimization, compared to the Flow Design.
> 
> It's recommended to start with the Naive Chunking and optimize later.
{: .best-practice }

---

## Example TypeScript Code Samples

### 1. Naive (Fixed-Size) Chunking
Splits text by a fixed number of characters, ignoring sentence or semantic boundaries.

```typescript
function fixedSizeChunk(text: string, chunkSize: number = 100): string[] {
  const chunks: string[] = [];
  for (let i = 0; i < text.length; i += chunkSize) {
    chunks.push(text.slice(i, i + chunkSize));
  }
  return chunks;
}
```

However, sentences are often cut awkwardly, losing coherence.

### 2. Sentence-Based Chunking

```typescript
import { tokenizeSentences } from "some-nlp-library"; // Replace with an actual NLP library

function sentenceBasedChunk(text: string, maxSentences: number = 2): string[] {
  const sentences = tokenizeSentences(text); // Tokenize text into sentences
  const chunks: string[] = [];
  for (let i = 0; i < sentences.length; i += maxSentences) {
    chunks.push(sentences.slice(i, i + maxSentences).join(" "));
  }
  return chunks;
}
```

However, this might not handle very long sentences or paragraphs well.

### 3. Other Chunking

- **Paragraph-Based**: Split text by paragraphs (e.g., newlines). Large paragraphs can create big chunks.
- **Semantic**: Use embeddings or topic modeling to chunk by semantic boundaries.
- **Agentic**: Use an LLM to decide chunk boundaries based on context or meaning.

```typescript
function paragraphBasedChunk(text: string): string[] {
  return text.split("\n\n"); // Split by double newlines
}

async function semanticChunk(text: string, maxTokens: number): Promise<string[]> {
  // Example: Use an LLM or embedding-based approach to determine semantic boundaries
  const chunks: string[] = [];
  // Implement semantic chunking logic here
  return chunks;
}

async function agenticChunk(text: string, prompt: string): Promise<string[]> {
  const response = await callLLM(`${prompt}\n${text}`);
  return response.split("\n\n"); // Example: Split response into chunks
}
```