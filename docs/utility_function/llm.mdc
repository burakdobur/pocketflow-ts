---
description: Guidelines for using PocketFlow, Utility Function, LLM Wrapper
globs: 
alwaysApply: false
---
# LLM Wrappers

Check out libraries like [litellm](https://github.com/BerriAI/litellm). 
Here, we provide some minimal example implementations:

1. OpenAI
    ```typescript
    import { Configuration, OpenAIApi } from "openai";

    const configuration = new Configuration({ apiKey: "YOUR_API_KEY_HERE" });
    const openai = new OpenAIApi(configuration);

    async function callLLM(prompt: string): Promise<string> {
      const response = await openai.createChatCompletion({
        model: "gpt-4",
        messages: [{ role: "user", content: prompt }],
      });
      return response.data.choices[0].message?.content || "";
    }

    // Example usage
    callLLM("How are you?").then(console.log);
    ```
    > Store the API key in an environment variable like `OPENAI_API_KEY` for security.
    {: .best-practice }

2. Claude (Anthropic)
    ```typescript
    import { Client } from "@anthropic-ai/sdk";

    const client = new Client("YOUR_API_KEY_HERE");

    async function callLLM(prompt: string): Promise<string> {
      const response = await client.completions.create({
        model: "claude-3",
        max_tokens_to_sample: 3000,
        prompt,
      });
      return response.completion;
    }
    ```

3. Google (Generative AI Studio / PaLM API)
    ```typescript
    import { GenerativeLanguageServiceClient } from "@google-cloud/generative-language";

    const client = new GenerativeLanguageServiceClient();

    async function callLLM(prompt: string): Promise<string> {
      const [response] = await client.generateText({
        model: "models/text-bison-001",
        prompt,
      });
      return response.candidates[0].output || "";
    }
    ```

4. Azure (Azure OpenAI)
    ```typescript
    import axios from "axios";

    async function callLLM(prompt: string): Promise<string> {
      const response = await axios.post(
        "https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15",
        { messages: [{ role: "user", content: prompt }] },
        { headers: { "api-key": "YOUR_API_KEY_HERE" } }
      );
      return response.data.choices[0].message.content;
    }
    ```

5. Ollama (Local LLM)
    ```typescript
    import { chat } from "ollama";

    async function callLLM(prompt: string): Promise<string> {
      const response = await chat({
        model: "llama2",
        messages: [{ role: "user", content: prompt }],
      });
      return response.message.content;
    }
    ```

6. DeepSeek
    ```typescript
    import axios from "axios";

    async function callLLM(prompt: string): Promise<string> {
      const response = await axios.post(
        "https://api.deepseek.com/chat/completions",
        { model: "deepseek-chat", messages: [{ role: "user", content: prompt }] },
        { headers: { Authorization: "Bearer YOUR_API_KEY_HERE" } }
      );
      return response.data.choices[0].message.content;
    }
    ```


## Improvements
Feel free to enhance your `callLLM` function as needed. Here are examples:

- Handle chat history:

```typescript
async function callLLM(messages: { role: string; content: string }[]): Promise<string> {
  const response = await openai.createChatCompletion({
    model: "gpt-4",
    messages,
  });
  return response.data.choices[0].message?.content || "";
}
```

- Add in-memory caching:

```typescript
const cache = new Map<string, string>();

async function callLLM(prompt: string, useCache: boolean = true): Promise<string> {
  if (useCache && cache.has(prompt)) {
    return cache.get(prompt) || "";
  }
  const response = await openai.createChatCompletion({
    model: "gpt-4",
    messages: [{ role: "user", content: prompt }],
  });
  const result = response.data.choices[0].message?.content || "";
  if (useCache) {
    cache.set(prompt, result);
  }
  return result;
}
```

> ⚠️ Caching conflicts with Node retries, as retries yield the same result.
>
> To address this, you could use cached results only if not retried.
{: .warning }

- Enable logging:

```typescript
async function callLLM(prompt: string): Promise<string> {
  console.log(`Prompt: ${prompt}`);
  const response = await openai.createChatCompletion({
    model: "gpt-4",
    messages: [{ role: "user", content: prompt }],
  });
  const result = response.data.choices[0].message?.content || "";
  console.log(`Response: ${result}`);
  return result;
}
```

